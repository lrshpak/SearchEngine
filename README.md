# cs2461Project5
## Report:
##Hashmap.c
In this file, I first create the hashmap using hm_create (). This allocates space for the number buckets and sets each of the buckets to NULL, and it sets num_elements to 0 and num_buckets to the number passed it. 
I then have hm_get() this function hashes the word passed into it and then it goes through the whole list at the bucket it hashed until it finds the word. If it finds the word, it returns the entire node, so it can be used in other functions. If it cannot find the node NULL is returned. 
hm_put() places a node. It hashes the word to get the bucket. If the word it is placing is not there and the bucket is empty, then it creates a new node and sets all the parameters. I have my node having a d1_occurence, a d2_occurence, and a d3_occurence, this way the word is only in the hashmap once, but the occurrences change based off the document. If the bucket is not null it just iterates through the hashmap looking for the word, if the word is there then nothing happens, because changing the occurrences happens in the search.c file. If the word is not there, then it creates a new node.
Hm_remove() removes a node based off the word. It first hashes to get the bucket. It then iterates until it finds the word keeping track of the index. If the iterator gets to the end of the list and it hasn’t found the node then it prints “no node”. If the index is 0 meaning it is the head node, then the functions removes the head and makes the next node the head node. It then frees the head. If the node to be removed is not the head node then it iterates through the list until it is one before the one to remove, it uses the index to find it. It then sets the node to remove to a temp node and the iterator bypasses this node by changing its next pointers. The temp is freed, so the node gets removed.
Hm_destroy() iterates through the entire hashmap freeing each node and bucket.
Int hash() is the function that determines the bucket. It iterates through the word passed in and adds the ASCII characters of each letter together then mods by the number of buckets. 
## Search.c
	In the main() I create an array of the file names. I also ask the user for the number of buckets and call training() to create the hashmap. I then call stopWords() to remove the stop words from the hashmap. I then ask the user if they would like to search or exit the program. If they input an X, I simply break and exit the program. If the user enters an S I, ask for a search query. I assumed the query won’t be more than 100 characters long. I then call readQuery(). The last thing I do in main() is destroy the hashmap. 
	In the training() function, I fill the hashmap. I use hm_create() to create the hashmap with the specified number of buckets. I then read through all the files and get the individual words from them. I call hm_get() on each word and if hm_get() returns NULL, then I put the word in the map, passing in 1 for the document occurrence for the document it is in. If hm_get() does not return NULL, then I update the occurrence based off the document. Since I set hm_get() equal to a node, I can directly update the occurrences without having to put the word in the hashmap again. I then call documentFrequency() and return the completed hashmap.
	documentFrequency() is called by training() in order to fill in the frequency for each word. This function goes through the hashmap and if the document occurrence for a word is greater than 0 then it adds one to the document frequency. I then directly updated the frequencies for each word.  I then return this completed hashmap to training().
	After I call training() in main, I call stopWords(). In this function, I iterate through every word in the hashmap and calculate the inverse document frequency. I use the node’s document frequency to do this. If the inverse document frequency for a word is 0 then I use hm_remove() to remove the word from the hashmap.
	After I do the training phase, I call readQuery() in main. This function goes through the query the user has put in and counts how many words there are, it does so by looking for a space or the string end character which is ‘\000’. I use the number of words to create an array of strings big enough, so all the words will fit. I then use strtok to get each individual word.  This function then calls rank().
	Rank is the last function I perform. I iterate through the array of words from the query and get them from the hashmap, if the word is not there, I set tf-idf to 0. This happens if the word searched is not in any documents or if the word searched is a stopWord(). If the node does exist then, it calculates the tf-idf for each document and adds to the sum for each document. The sum is a running sum so by the end of going through the query, the sum is the total tf-idf score for that document. To sort the documents, I have an array of the file names in the same order as my array of scores. I then go through the arrays and compare the td-idf scores. If second score is bigger than the first score, then it swaps the documents in the document array. I then print out the ordered list. 
*Also, just a note: I ran my program on shell01 and got no errors. On shell, I get errors, but the program still runs and works properly.
